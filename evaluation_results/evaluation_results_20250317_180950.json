{
  "timestamp": "2025-03-17T18:09:50.949787",
  "num_examples": 5,
  "aggregated_metrics": {
    "context_relevance": {
      "mean": 0.6391383962933779,
      "median": 0.648790048452121,
      "min": 0.5397441024347357,
      "max": 0.6852750721349337,
      "std": 0.052952534342063706
    },
    "context_utilization": {
      "mean": 0.5795724019653286,
      "median": 0.5625,
      "min": 0.39285714285714285,
      "max": 0.8260869565217391,
      "std": 0.17140386299045962
    },
    "semantic_similarity": {
      "mean": 0.763626438960012,
      "median": 0.7406012082544113,
      "min": 0.6655413282167895,
      "max": 0.8533571553671423,
      "std": 0.07057293538679979
    },
    "bleu_score": {
      "mean": 0.08527194460610965,
      "median": 0.11002068284361832,
      "min": 0.01214685461458544,
      "max": 0.16020720994064927,
      "std": 0.05748973887431611
    },
    "potential_hallucination_score": {
      "mean": 0.4204275980346714,
      "median": 0.4375,
      "min": 0.17391304347826086,
      "max": 0.6071428571428572,
      "std": 0.17140386299045965
    },
    "rouge2_f1": {
      "mean": 0.14398522914271544,
      "median": 0.14814814814814814,
      "min": 0.043750000000000004,
      "max": 0.25,
      "std": 0.08559309781464972
    },
    "rougeL_f1": {
      "mean": 0.2535468823616151,
      "median": 0.25,
      "min": 0.09937888198757765,
      "max": 0.39999999999999997,
      "std": 0.1044663867994549
    },
    "answer_length": {
      "mean": 87.4,
      "median": 28.0,
      "min": 18,
      "max": 259,
      "std": 91.90777986655971
    },
    "rouge1_f1": {
      "mean": 0.29176405427859314,
      "median": 0.2857142857142857,
      "min": 0.10559006211180123,
      "max": 0.39999999999999997,
      "std": 0.10836546202035906
    }
  },
  "individual_results": [
    {
      "question": "Qu'est-ce qu'un perceptron en Deep Learning ?",
      "generated_answer": "Un perceptron est un modÃ¨le de rÃ©seau neuronal qui est utilisÃ© pour identifier des Ã©lÃ©ments dans une image.",
      "metrics": {
        "rouge1_f1": 0.39999999999999997,
        "rouge2_f1": 0.25,
        "rougeL_f1": 0.39999999999999997,
        "bleu_score": 0.11993823638921594,
        "semantic_similarity": 0.8350293735715083,
        "context_utilization": 0.5625,
        "potential_hallucination_score": 0.4375,
        "context_relevance": 0.6852750721349337,
        "answer_length": 18
      },
      "reference_answer": "Un perceptron est un rÃ©seau neuronal constituÃ© d'un seul neurone, qui prend un vecteur en entrÃ©e et applique une fonction d'activation pour produire une sortie.",
      "retrieved_context": "[Extrait 1 - Source: book_fr.pdf (page 19)]\nIntroduction au Deep Learning (notes de cours)\n16 Chapitre 2. Perceptrons multicouches\n\n[Extrait 2 - Source: book_fr.pdf (page 19)]\nIntroduction au Deep Learning (notes de cours)\n16 Chapitre 2. Perceptrons multicouches\n\n[Extrait 3 - Source: book_fr.pdf (page 6)]\nCHAPITRE 1\nINTRODUCTION\nDans ce chapitre dâ€™introduction, nous allons prÃ©senter un premier rÃ©seau neuronal appelÃ© le Perceptron. Ce modÃ¨le est un\nrÃ©seau neuronal constituÃ© dâ€™un seul neurone, et nous lâ€™utiliserons ici pour introduire des concepts-clÃ©s que nous dÃ©taillerons\nplus tard dans le cours.\n1.1...\n\n[Extrait 4 - Source: book_fr.pdf (page 6)]\nCHAPITRE 1\nINTRODUCTION\nDans ce chapitre dâ€™introduction, nous allons prÃ©senter un premier rÃ©seau neuronal appelÃ© le Perceptron. Ce modÃ¨le est un\nrÃ©seau neuronal constituÃ© dâ€™un seul neurone, et nous lâ€™utiliserons ici pour introduire des concepts-clÃ©s que nous dÃ©taillerons\nplus tard dans le cours.\n1.1...\n\n[Extrait 5 - Source: book_fr.pdf (page 2)]\nTABLE DES MATIÃˆRES\n1 Introduction 3\n1.1 Un premier modÃ¨le : le perceptron. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .3\n1.2 Optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4\n1.3 RÃ©capitulatif . . . . . . . . . . . . ...",
      "id": "example_0",
      "metadata": {}
    },
    {
      "question": "Pourquoi utilise-t-on la descente de gradient en Deep Learning ?",
      "generated_answer": "* La descente de gradient est un algorithme bien adaptÃ© pour les modÃ¨les plus complexes que les courbes de perte.\n* Cet algorithme est plus efficace et plus rapide que dâ€™autres algorithmes pour la mÃªme quantitÃ© de donnÃ©es.\n* Il est plus simple dâ€™utiliser et de comprendre que les rÃ©seaux de neurones.\n\n[Extrait 6 - Source: book_fr.pdf (page 10)]\nIntroduction au Deep Learning (notes de cours)\nOn peut alors utiliser les mÃªmes mÃ©thodes pour calculer la perte pour un modÃ¨le de Deep Learning. Pour cela, on peut\nutiliser les mÃªmes rÃ©seaux de neurones que prÃ©cÃ©demment, mais on utilise des fonctions dâ€™activation diffÃ©rentes.\n\n[Extrait 7 - Source: book_fr.pdf (page 10)]\nIntroduction au Deep Learning (notes de cours)\nOn peut alors utiliser les mÃªmes mÃ©thodes pour calculer la perte pour un modÃ¨le de Deep Learning. Pour cela, on peut\nutiliser les mÃªmes rÃ©seaux de neurones que prÃ©cÃ©demment, mais on utilise des fonctions dâ€™activation diffÃ©rentes.\n\n[Extrait 8 - Source: book_fr.pdf (page 11)]\nIntroduction au Deep Learning (notes de cours)\n4.1 Descente de gradient stochastique\nDans notre cas, on utilise une fonction de perte qui est la fonction de L2 (la norme de la diffÃ©rence entre la valeur de\nla fonction de perte et la valeur de la fonction de perte estimÃ©e).\n\n[Extrait 9 - Source: book_fr.pdf (page 11)]\nIntroduction au Deep Learning (notes de cours)\n4.1 Descente de gradient stochastique\nDans notre cas, on utilise une fonction de perte qui est la fonction de L2 (la norme de la diffÃ©rence entre la valeur de\nla fonction de perte et la valeur de",
      "metrics": {
        "rouge1_f1": 0.10559006211180123,
        "rouge2_f1": 0.043750000000000004,
        "rougeL_f1": 0.09937888198757765,
        "bleu_score": 0.024046739242479305,
        "semantic_similarity": 0.7236031293902092,
        "context_utilization": 0.39285714285714285,
        "potential_hallucination_score": 0.6071428571428572,
        "context_relevance": 0.6829429864167683,
        "answer_length": 259
      },
      "reference_answer": "La descente de gradient est utilisÃ©e pour minimiser la fonction de perte en mettant Ã  jour les poids du rÃ©seau de maniÃ¨re itÃ©rative dans la direction du gradient nÃ©gatif.",
      "retrieved_context": "[Extrait 1 - Source: book_fr.pdf (page 7)]\nIntroduction au Deep Learning (notes de cours)\n1.2 Optimisation\nLes modÃ¨les prÃ©sentÃ©s dans ce document ont pour but de rÃ©soudre des problÃ¨mes de prÃ©diction dans lesquels lâ€™objectif est\nde trouver des valeurs de paramÃ¨tres Â« suffisamment bonnes Â» pour le modÃ¨le en jeu compte tenu de donnÃ©es observÃ©es...\n\n[Extrait 2 - Source: book_fr.pdf (page 7)]\nIntroduction au Deep Learning (notes de cours)\n1.2 Optimisation\nLes modÃ¨les prÃ©sentÃ©s dans ce document ont pour but de rÃ©soudre des problÃ¨mes de prÃ©diction dans lesquels lâ€™objectif est\nde trouver des valeurs de paramÃ¨tres Â« suffisamment bonnes Â» pour le modÃ¨le en jeu compte tenu de donnÃ©es observÃ©es...\n\n[Extrait 3 - Source: book_fr.pdf (page 23)]\nIntroduction au Deep Learning (notes de cours)\n4.1 Descente de gradient stochastique\nLâ€™idÃ©e derriÃ¨re lâ€™algorithme de descente de gradient stochastique (ouStochastic Gradient Descent, SGD) est dâ€™obtenir des\nestimations bon marchÃ© (au sens de la quantitÃ© de calculs nÃ©cessaires) pour la quantitÃ©\nâˆ‡ğœƒ â„’(ğ’Ÿ...\n\n[Extrait 4 - Source: book_fr.pdf (page 23)]\nIntroduction au Deep Learning (notes de cours)\n4.1 Descente de gradient stochastique\nLâ€™idÃ©e derriÃ¨re lâ€™algorithme de descente de gradient stochastique (ouStochastic Gradient Descent, SGD) est dâ€™obtenir des\nestimations bon marchÃ© (au sens de la quantitÃ© de calculs nÃ©cessaires) pour la quantitÃ©\nâˆ‡ğœƒ â„’(ğ’Ÿ...\n\n[Extrait 5 - Source: book_fr.pdf (page 9)]\nIntroduction au Deep Learning (notes de cours)\nIci, il semble quâ€™une valeur değ‘¤ autour de 4 devrait Ãªtre un bon choix. Cette mÃ©thode (gÃ©nÃ©rer de nombreuses valeurs pour\nle paramÃ¨tre et calculer la perte pour chaque valeur) ne peut pas sâ€™adapter aux modÃ¨les qui ont beaucoup de paramÃ¨tres,\ndonc nous a...",
      "id": "example_1",
      "metadata": {}
    },
    {
      "question": "Quelle est la principale fonction de perte utilisÃ©e pour la rÃ©gression ?",
      "generated_answer": "La perte logistique est la plus couramment utilisÃ©e pour la rÃ©gression. Elle est largement utilisÃ©e pour\nrÃ©gler les problÃ¨mes de classification.\n\n[Extrait 6 - Source: book_fr.pdf (page 21)]",
      "metrics": {
        "rouge1_f1": 0.39999999999999997,
        "rouge2_f1": 0.22641509433962265,
        "rougeL_f1": 0.32727272727272727,
        "bleu_score": 0.11002068284361832,
        "semantic_similarity": 0.6655413282167895,
        "context_utilization": 0.8260869565217391,
        "potential_hallucination_score": 0.17391304347826086,
        "context_relevance": 0.6389397720283309,
        "answer_length": 28
      },
      "reference_answer": "L'erreur quadratique moyenne (Mean Squared Error, MSE) est la fonction de perte la plus couramment utilisÃ©e en rÃ©gression.",
      "retrieved_context": "[Extrait 1 - Source: book_fr.pdf (page 20)]\ndÃ©signons la sortie du modÃ¨le :\nâˆ€ğ‘–, Ì‚ ğ‘¦ğ‘– = ğ‘šğœƒ (ğ‘¥ğ‘– )\noÃ¹ ğ‘šğœƒ est notre modÃ¨le etğœƒ est lâ€™ensemble de tous ses paramÃ¨tres (poids et biais).\n3.1 Erreur quadratique moyenne\nLâ€™erreur quadratique moyenne (ouMean Squared Error, MSE) est la fonction de perte la plus couramment utilisÃ©e dans\nles contextes de rÃ©...\n\n[Extrait 2 - Source: book_fr.pdf (page 20)]\ndÃ©signons la sortie du modÃ¨le :\nâˆ€ğ‘–, Ì‚ ğ‘¦ğ‘– = ğ‘šğœƒ (ğ‘¥ğ‘– )\noÃ¹ ğ‘šğœƒ est notre modÃ¨le etğœƒ est lâ€™ensemble de tous ses paramÃ¨tres (poids et biais).\n3.1 Erreur quadratique moyenne\nLâ€™erreur quadratique moyenne (ouMean Squared Error, MSE) est la fonction de perte la plus couramment utilisÃ©e dans\nles contextes de rÃ©...\n\n[Extrait 3 - Source: book_fr.pdf (page 11)]\nâ€” le fait quâ€™une tÃ¢che sâ€™accompagne dâ€™une fonction de perte Ã  minimiser (ici, nous avons utilisÃ© lâ€™erreur quadratique\nmoyenne pour notre tÃ¢che de rÃ©gression), qui sera discutÃ©e dansun chapitre dÃ©diÃ©;\nâ€” le concept de descente de gradient pour optimiser la perte choisie sur le paramÃ¨tre unique dâ€™un mo...\n\n[Extrait 4 - Source: book_fr.pdf (page 11)]\nâ€” le fait quâ€™une tÃ¢che sâ€™accompagne dâ€™une fonction de perte Ã  minimiser (ici, nous avons utilisÃ© lâ€™erreur quadratique\nmoyenne pour notre tÃ¢che de rÃ©gression), qui sera discutÃ©e dansun chapitre dÃ©diÃ©;\nâ€” le concept de descente de gradient pour optimiser la perte choisie sur le paramÃ¨tre unique dâ€™un mo...\n\n[Extrait 5 - Source: book_fr.pdf (page 21)]\nIntroduction au Deep Learning (notes de cours)\n3.2 Perte logistique\nLa perte logistique est la fonction de perte la plus largement utilisÃ©e pour entraÃ®ner des rÃ©seaux neuronaux dans des\ncontextes de classification. Elle est dÃ©finie comme suit\nâ„’(ğ’Ÿ; ğ‘šğœƒ ) = 1\nğ‘› âˆ‘\nğ‘–\nâˆ’ log ğ‘( Ì‚ ğ‘¦ğ‘– = ğ‘¦ğ‘– ; ğ‘šğœƒ )\noÃ¹ ğ‘( Ì‚ ğ‘¦ğ‘– ...",
      "id": "example_2",
      "metadata": {}
    },
    {
      "question": "Quels sont les avantages des rÃ©seaux de neurones convolutifs (CNN) ?",
      "generated_answer": "Les rÃ©seaux de neurones convolutifs (CNN) offrent un avantage trÃ¨s important dans le domaine de la\nvisualisation spatiale. Ils sont trÃ¨s efficaces pour la reconnaissance de formes et les gestes. En raison de\ncontraintes de temps, les rÃ©seaux de neurones convolutifs (CNN) sont Ã©galement trÃ¨s efficaces pour la reconnaissance\ndâ€™images et les vidÃ©os.\n\n[Extrait 6 - Source: book_fr.pdf (page 53)]\nIntroduction au Deep Learning (notes de cours)\n7.5 Conclusion\nDans ce chapitre et le prÃ©cÃ©dent, nous avons passÃ© en revue les architectures de rÃ©seaux de neurones qui sont utilisÃ©es\npour apprendre Ã  partir de donnÃ©es temporelles ou sÃ©quentielles. En raison de contraintes de temps, nous nâ€™avons pas\na...",
      "metrics": {
        "rouge1_f1": 0.267515923566879,
        "rouge2_f1": 0.051612903225806445,
        "rougeL_f1": 0.1910828025477707,
        "bleu_score": 0.01214685461458544,
        "semantic_similarity": 0.7406012082544113,
        "context_utilization": 0.7164179104477612,
        "potential_hallucination_score": 0.28358208955223885,
        "context_relevance": 0.648790048452121,
        "answer_length": 108
      },
      "reference_answer": "Les CNN exploitent la structure spatiale des donnÃ©es, rÃ©duisent le nombre de paramÃ¨tres Ã  apprendre et sont efficaces pour la classification d'images grÃ¢ce aux couches de convolution et de pooling.",
      "retrieved_context": "[Extrait 1 - Source: book_fr.pdf (page 36)]\nCHAPITRE 6\nRÃ‰SEAUX NEURONAUX CONVOLUTIFS\nLes rÃ©seaux de neurones convolutifs (aussi appelÃ©s ConvNets) sont conÃ§us pour tirer parti de la structure des donnÃ©es.\nDans ce chapitre, nous aborderons deux types de rÃ©seaux convolutifs : nous commencerons par le cas monodimensionnel\net verrons comment les r...\n\n[Extrait 2 - Source: book_fr.pdf (page 36)]\nCHAPITRE 6\nRÃ‰SEAUX NEURONAUX CONVOLUTIFS\nLes rÃ©seaux de neurones convolutifs (aussi appelÃ©s ConvNets) sont conÃ§us pour tirer parti de la structure des donnÃ©es.\nDans ce chapitre, nous aborderons deux types de rÃ©seaux convolutifs : nous commencerons par le cas monodimensionnel\net verrons comment les r...\n\n[Extrait 3 - Source: book_fr.pdf (page 43)]\nIntroduction au Deep Learning (notes de cours)\n40 Chapitre 6. RÃ©seaux neuronaux convolutifs\n\n[Extrait 4 - Source: book_fr.pdf (page 43)]\nIntroduction au Deep Learning (notes de cours)\n40 Chapitre 6. RÃ©seaux neuronaux convolutifs\n\n[Extrait 5 - Source: book_fr.pdf (page 48)]\nIntroduction au Deep Learning (notes de cours)\n7.4 Conclusion\nDans ce chapitre et le prÃ©cÃ©dent, nous avons passÃ© en revue les architectures de rÃ©seaux de neurones qui sont utilisÃ©es\npour apprendre Ã  partir de donnÃ©es temporelles ou sÃ©quentielles. En raison de contraintes de temps, nous nâ€™avons pas\na...",
      "id": "example_3",
      "metadata": {}
    },
    {
      "question": "En quoi consiste la rÃ©gularisation DropOut ?",
      "generated_answer": "La rÃ©gularisation DropOut consiste Ã  faire varier la valeur de la fonction dâ€™activation de faÃ§on Ã  ce que le\nsous-rÃ©seau rÃ©sultant soit plus robuste.",
      "metrics": {
        "rouge1_f1": 0.2857142857142857,
        "rouge2_f1": 0.14814814814814814,
        "rougeL_f1": 0.25,
        "bleu_score": 0.16020720994064927,
        "semantic_similarity": 0.8533571553671423,
        "context_utilization": 0.4,
        "potential_hallucination_score": 0.6,
        "context_relevance": 0.5397441024347357,
        "answer_length": 24
      },
      "reference_answer": "La rÃ©gularisation DropOut consiste Ã  dÃ©sactiver alÃ©atoirement des neurones pendant l'apprentissage pour Ã©viter le surajustement et amÃ©liorer la gÃ©nÃ©ralisation du modÃ¨le.",
      "retrieved_context": "[Extrait 1 - Source: book_fr.pdf (page 33)]\nIntroduction au Deep Learning (notes de cours)\nfrom keras.regularizers import L2\nÎ» = 0.01\nset_random_seed(0)\nmodel = Sequential([\nInputLayer(input_shape=(4, )),\nDense(units=256, activation=\"relu\", kernel_regularizer=L2(Î»)),\nDense(units=256, activation=\"relu\", kernel_regularizer=L2(Î»)),\nDense(units=2...\n\n[Extrait 2 - Source: book_fr.pdf (page 33)]\nIntroduction au Deep Learning (notes de cours)\nfrom keras.regularizers import L2\nÎ» = 0.01\nset_random_seed(0)\nmodel = Sequential([\nInputLayer(input_shape=(4, )),\nDense(units=256, activation=\"relu\", kernel_regularizer=L2(Î»)),\nDense(units=256, activation=\"relu\", kernel_regularizer=L2(Î»)),\nDense(units=2...\n\n[Extrait 3 - Source: book_fr.pdf (page 34)]\nIntroduction au Deep Learning (notes de cours)\nFIG. 5.1 â€“ Illustration du mÃ©canisme deDropOut. Afin dâ€™entraÃ®ner un modÃ¨le donnÃ© (Ã  gauche), Ã  chaqueminibatch, une\nproportion donnÃ©e de neurones est choisie au hasard pour Ãªtre Â« dÃ©sactivÃ©e Â» et le sous-rÃ©seau rÃ©sultant est utilisÃ© pour\nlâ€™Ã©tape dâ€™optim...\n\n[Extrait 4 - Source: book_fr.pdf (page 34)]\nIntroduction au Deep Learning (notes de cours)\nFIG. 5.1 â€“ Illustration du mÃ©canisme deDropOut. Afin dâ€™entraÃ®ner un modÃ¨le donnÃ© (Ã  gauche), Ã  chaqueminibatch, une\nproportion donnÃ©e de neurones est choisie au hasard pour Ãªtre Â« dÃ©sactivÃ©e Â» et le sous-rÃ©seau rÃ©sultant est utilisÃ© pour\nlâ€™Ã©tape dâ€™optim...\n\n[Extrait 5 - Source: book_fr.pdf (page 2)]\n5 RÃ©gularisation 27\n5.1 Early stopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .27\n5.2 PÃ©nalisation de la perte. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .29\n5.3 DropOut . . . . . . . . . . . . . . . . . . . . ...",
      "id": "example_4",
      "metadata": {}
    }
  ]
}